{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Lambda\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_neg(y):\n",
    "    y_neg = y.clone()\n",
    "    for idx, y_samp in enumerate(y):\n",
    "        allowed_indices = list(range(10))\n",
    "        allowed_indices.remove(y_samp.item())\n",
    "        y_neg[idx] = torch.tensor(allowed_indices)[\n",
    "            torch.randint(len(allowed_indices), size=(1,))\n",
    "        ].item()\n",
    "    return y_neg.to(device)\n",
    "\n",
    "\n",
    "def overlay_y_on_x(x, y, classes=10):\n",
    "    x_ = x.clone()\n",
    "    x_[:, :classes] *= 0.0\n",
    "    x_[range(x.shape[0]), y] = x.max()\n",
    "    return x_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, dims):\n",
    "\n",
    "        super().__init__()\n",
    "        self.layers = []\n",
    "        for d in range(len(dims) - 1):\n",
    "            self.layers = self.layers + [Layer(dims[d], dims[d + 1]).to(device)]\n",
    "\n",
    "    def predict(self, x):\n",
    "        goodness_per_label = []\n",
    "        for label in range(10):\n",
    "            h = overlay_y_on_x(x, label)\n",
    "            goodness = []\n",
    "            for layer in self.layers:\n",
    "                h = layer(h)\n",
    "                goodness = goodness + [h.pow(2).mean(1)]\n",
    "            goodness_per_label += [sum(goodness).unsqueeze(1)]\n",
    "        goodness_per_label = torch.cat(goodness_per_label, 1)\n",
    "        return goodness_per_label.argmax(1)\n",
    "\n",
    "    def train(self, x_pos, x_neg):\n",
    "        h_pos, h_neg = x_pos, x_neg\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print(\"training layer: \", i)\n",
    "            h_pos, h_neg = layer.train(h_pos, h_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, bias=True, device=None, dtype=None):\n",
    "        super().__init__(in_features, out_features, bias, device, dtype)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.opt = Adam(self.parameters(), lr=0.01)\n",
    "        self.threshold = 0.5\n",
    "        self.num_epochs = 20\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_direction = x / (x.norm(2, 1, keepdim=True) + 1e-4)\n",
    "        return self.relu(torch.mm(x_direction, self.weight.T) + self.bias.unsqueeze(0))\n",
    "\n",
    "    def train(self, x_pos, x_neg):\n",
    "        for i in range(self.num_epochs):\n",
    "            g_pos = self.forward(x_pos).pow(2).mean(1)\n",
    "            g_neg = self.forward(x_neg).pow(2).mean(1)\n",
    "            loss = torch.log1p(\n",
    "                torch.exp(\n",
    "                    torch.cat([-g_pos + self.threshold, g_neg - self.threshold])\n",
    "                )\n",
    "            ).mean()\n",
    "            self.opt.zero_grad()\n",
    "            loss.backward()\n",
    "            self.opt.step()\n",
    "            print(\"Loss: \", loss.item())\n",
    "        return self.forward(x_pos).detach(), self.forward(x_neg).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "use_mps = torch.backends.mps.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "elif use_mps:\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "train_kwargs = {\"batch_size\": 32}\n",
    "test_kwargs = {\"batch_size\": 32}\n",
    "\n",
    "if use_cuda:\n",
    "    cuda_kwargs = {\"num_workers\": 1, \"pin_memory\": True, \"shuffle\": True}\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    test_kwargs.update(cuda_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose(\n",
    "    [\n",
    "        ToTensor(),\n",
    "        Normalize((0.1307,), (0.3081,)),\n",
    "        Lambda(lambda x: torch.flatten(x)),\n",
    "    ]\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    MNIST(\"./data/\", train=True, download=True, transform=transform), **train_kwargs\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    MNIST(\"./data/\", train=False, download=True, transform=transform), **test_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training layer:  0\n",
      "Loss:  0.7240239381790161\n",
      "Loss:  0.7223004102706909\n",
      "Loss:  0.7182900905609131\n",
      "Loss:  0.7118580341339111\n",
      "Loss:  0.7039902806282043\n",
      "Loss:  0.6968176364898682\n",
      "Loss:  0.6937904953956604\n",
      "Loss:  0.6971739530563354\n",
      "Loss:  0.7003540992736816\n",
      "Loss:  0.6988317370414734\n",
      "Loss:  0.6952486038208008\n",
      "Loss:  0.6924381256103516\n",
      "Loss:  0.6914573311805725\n",
      "Loss:  0.691879153251648\n",
      "Loss:  0.6927556395530701\n",
      "Loss:  0.6933597326278687\n",
      "Loss:  0.6933772563934326\n",
      "Loss:  0.6928091645240784\n",
      "Loss:  0.691849946975708\n",
      "Loss:  0.6908039450645447\n",
      "training layer:  1\n",
      "Loss:  0.7239922881126404\n",
      "Loss:  0.7204704880714417\n",
      "Loss:  0.7137596607208252\n",
      "Loss:  0.7045049667358398\n",
      "Loss:  0.6958726644515991\n",
      "Loss:  0.6934424638748169\n",
      "Loss:  0.6998155117034912\n",
      "Loss:  0.7017667293548584\n",
      "Loss:  0.6981872320175171\n",
      "Loss:  0.6944157481193542\n",
      "Loss:  0.6930897831916809\n",
      "Loss:  0.69392329454422\n",
      "Loss:  0.6954567432403564\n",
      "Loss:  0.6965612173080444\n",
      "Loss:  0.696794331073761\n",
      "Loss:  0.6961874961853027\n",
      "Loss:  0.6950468420982361\n",
      "Loss:  0.6938498020172119\n",
      "Loss:  0.6931235194206238\n",
      "Loss:  0.6932017803192139\n",
      "train error: 0.8125\n",
      "test error: 0.84375\n"
     ]
    }
   ],
   "source": [
    "net = Net([784, 500, 500])\n",
    "\n",
    "x, y = next(iter(train_loader))\n",
    "x, y = x.to(device), y.to(device)\n",
    "\n",
    "x_pos = overlay_y_on_x(x, y)\n",
    "y_neg = get_y_neg(y)\n",
    "x_neg = overlay_y_on_x(x, y_neg)\n",
    "net.train(x_pos, x_neg)\n",
    "\n",
    "print(\"train error:\", 1.0 - net.predict(x).eq(y).float().mean().item())\n",
    "x_te, y_te = next(iter(test_loader))\n",
    "x_te, y_te = x_te.to(device), y_te.to(device)\n",
    "print(\"test error:\", 1.0 - net.predict(x_te).eq(y_te).float().mean().item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
